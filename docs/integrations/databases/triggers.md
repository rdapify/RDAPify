 # üéØ Database Triggers Integration Guide

> **üéØ Purpose:** Complete guide to implementing database triggers for RDAP data processing, compliance, and real-time analytics  
> **üìö Related:** [Database Schemas](schemas.md) | [Sync Tools](sync_tools.md) | [Security Privacy](../../guides/security_privacy.md)  
> **‚è±Ô∏è Reading Time:** 7 minutes  
> **üîç Pro Tip:** Use the [Trigger Validator](./trigger-validator.md) to automatically check your trigger configurations for security and performance best practices

---

## üåê Why Triggers Matter for RDAP Data Processing

Database triggers provide critical automation for RDAP data workflows, enabling real-time processing without application-level polling. Their importance spans multiple domains:

```mermaid
graph LR
    A[RDAP Client] --> B[Database]
    B --> C{Trigger Events}
    C --> D[PII Redaction]
    C --> E[Audit Logging]
    C --> F[Data Enrichment]
    C --> G[Compliance Checks]
    D --> H[Clean Data Store]
    E --> I[Audit Trail]
    F --> J[Analytics Engine]
    G --> K[Compliance Reports]
    
    style A fill:#4CAF50
    style B fill:#2196F3
    style C fill:#FF9800
    style D,E,F,G fill:#9C27B0
```

**Critical Trigger Use Cases:**
- ‚úÖ **Automatic PII Redaction**: Real-time anonymization of personal data before storage
- ‚úÖ **Compliance Enforcement**: GDPR/CCPA-mandated data retention and deletion
- ‚úÖ **Data Quality Enforcement**: Validation rules and automatic correction
- ‚úÖ **Change Data Capture**: Real-time synchronization to downstream systems
- ‚úÖ **Business Rule Execution**: Domain monitoring alerts, expiration notifications
- ‚úÖ **Audit Trail Generation**: Immutable logs of data modifications for compliance

---

## ‚öôÔ∏è Core Trigger Patterns

### 1. PII Redaction Triggers
```sql
-- PostgreSQL trigger for automatic PII redaction
CREATE OR REPLACE FUNCTION redact_pii()
RETURNS TRIGGER AS $$
BEGIN
  -- Redact email addresses
  NEW.registrant_email := 'REDACTED@' || split_part(NEW.registrant_email, '@', 2);
  NEW.tech_email := 'REDACTED@' || split_part(NEW.tech_email, '@', 2);
  
  -- Redact phone numbers
  NEW.registrant_phone := 'REDACTED';
  NEW.admin_phone := 'REDACTED';
  
  -- Redact names based on compliance level
  IF NEW.compliance_level = 'strict' THEN
    NEW.registrant_name := 'REDACTED';
    NEW.admin_name := 'REDACTED';
    NEW.tech_name := 'REDACTED';
  ELSE
    -- Basic redaction preserves organization names
    NEW.registrant_name := REGEXP_REPLACE(NEW.registrant_name, '\m[A-Za-z]+\M', 'REDACTED', 'g');
  END IF;
  
  -- Log redaction for audit
  INSERT INTO redaction_log (table_name, record_id, redaction_time, fields_redacted)
  VALUES (TG_TABLE_NAME, NEW.id, NOW(), ARRAY['email', 'phone', 'name']);
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Create trigger on domain table
CREATE TRIGGER trigger_redact_pii
BEFORE INSERT OR UPDATE ON domains
FOR EACH ROW EXECUTE FUNCTION redact_pii();

-- Index for performance
CREATE INDEX idx_domains_compliance_level ON domains(compliance_level);
```

### 2. GDPR/CCPA Auto-Deletion Triggers
```sql
-- MySQL event scheduler for GDPR data expiration
DELIMITER $$

CREATE EVENT cleanup_expired_data
ON SCHEDULE EVERY 1 DAY
DO
BEGIN
  -- Delete domain records older than retention period
  DELETE FROM domains 
  WHERE created_at < NOW() - INTERVAL (
    SELECT retention_days 
    FROM retention_policies 
    WHERE data_type = 'domain'
  ) DAY;
  
  -- Delete contact records with no references
  DELETE FROM contacts
  WHERE id NOT IN (
    SELECT DISTINCT contact_id FROM domain_contacts
  )
  AND created_at < NOW() - INTERVAL 30 DAY;
  
  -- Log deletions for compliance
  INSERT INTO data_retention_log (table_name, records_deleted, execution_time)
  VALUES ('domains', ROW_COUNT(), NOW());
  
END$$

DELIMITER ;

-- PostgreSQL equivalent using pg_cron
SELECT cron.schedule('cleanup_expired_data', '0 2 * * *', $$
  WITH deleted_domains AS (
    DELETE FROM domains 
    WHERE created_at < NOW() - INTERVAL '30 days'
    RETURNING id
  )
  INSERT INTO data_retention_log (table_name, records_deleted, execution_time)
  SELECT 'domains', COUNT(*), NOW() FROM deleted_domains;
$$);
```

### 3. Data Quality Enforcement Triggers
```sql
-- PostgreSQL trigger for domain validation
CREATE OR REPLACE FUNCTION validate_domain_data()
RETURNS TRIGGER AS $$
DECLARE
  invalid_tld BOOLEAN;
  missing_required BOOLEAN;
BEGIN
  -- Validate TLD against approved list
  SELECT NOT EXISTS (
    SELECT 1 FROM approved_tlds WHERE tld = split_part(NEW.domain_name, '.', -1)
  ) INTO invalid_tld;
  
  -- Check required fields
  missing_required := (NEW.registrar_id IS NULL OR NEW.nameservers IS NULL OR jsonb_array_length(NEW.nameservers) = 0);
  
  -- Raise exception for invalid data
  IF invalid_tld THEN
    RAISE EXCEPTION 'Invalid TLD for domain %', NEW.domain_name
    USING HINT = 'Domain must use approved TLDs from the approved_tlds table';
  END IF;
  
  IF missing_required THEN
    RAISE EXCEPTION 'Missing required fields for domain %', NEW.domain_name
    USING HINT = 'Registrar ID and at least one nameserver are required';
  END IF;
  
  -- Auto-correct common issues
  NEW.domain_name := LOWER(TRIM(NEW.domain_name));
  NEW.normalized_name := REGEXP_REPLACE(NEW.domain_name, '^www\.', '');
  
  -- Set quality score
  NEW.quality_score := CASE
    WHEN NEW.last_update_source = 'registry' THEN 100
    WHEN NEW.last_update_source = 'bootstrap' THEN 80
    ELSE 60
  END;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger
CREATE TRIGGER trigger_validate_domain_data
BEFORE INSERT OR UPDATE ON domains
FOR EACH ROW EXECUTE FUNCTION validate_domain_data();

-- Create approved TLDs table
CREATE TABLE approved_tlds (
  tld VARCHAR(10) PRIMARY KEY,
  registry VARCHAR(50) NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Sample approved TLDs
INSERT INTO approved_tlds (tld, registry) VALUES
('com', 'verisign'),
('org', 'pir'),
('net', 'verisign'),
('io', 'afilias'),
('dev', 'google');
```

---

## üîê Security & Compliance Triggers

### 1. Audit Trail Generator
```sql
-- PostgreSQL audit trigger
CREATE TABLE audit_log (
  id BIGSERIAL PRIMARY KEY,
  table_name TEXT NOT NULL,
  operation TEXT NOT NULL CHECK (operation IN ('INSERT', 'UPDATE', 'DELETE')),
  record_id UUID,
  old_data JSONB,
  new_data JSONB,
  changed_by TEXT,
  changed_ip INET,
  user_agent TEXT,
  changed_at TIMESTAMPTZ DEFAULT NOW(),
  legal_basis TEXT NOT NULL,
  compliance_framework TEXT NOT NULL
);

CREATE OR REPLACE FUNCTION create_audit_log()
RETURNS TRIGGER AS $$
DECLARE
  username TEXT;
  client_ip INET;
  user_agent TEXT;
  legal_basis TEXT;
BEGIN
  -- Get session variables
  username := current_setting('app.current_user', true);
  client_ip := current_setting('app.client_ip', true)::INET;
  user_agent := current_setting('app.user_agent', true);
  legal_basis := current_setting('app.legal_basis', true);
  
  -- Fallback to database user if application context not set
  IF username IS NULL THEN
    username := current_user;
  END IF;
  
  IF legal_basis IS NULL THEN
    legal_basis := 'legitimate-interest';
  END IF;
  
  -- Create audit record
  IF (TG_OP = 'DELETE') THEN
    INSERT INTO audit_log (
      table_name, operation, record_id, old_data, 
      changed_by, changed_ip, user_agent, 
      legal_basis, compliance_framework
    ) VALUES (
      TG_TABLE_NAME, 'DELETE', OLD.id, to_jsonb(OLD),
      username, client_ip, user_agent,
      legal_basis, 'gdpr'
    );
    RETURN OLD;
    
  ELSIF (TG_OP = 'UPDATE') THEN
    INSERT INTO audit_log (
      table_name, operation, record_id, old_data, new_data,
      changed_by, changed_ip, user_agent,
      legal_basis, compliance_framework
    ) VALUES (
      TG_TABLE_NAME, 'UPDATE', NEW.id, to_jsonb(OLD), to_jsonb(NEW),
      username, client_ip, user_agent,
      legal_basis, 'gdpr'
    );
    RETURN NEW;
    
  ELSIF (TG_OP = 'INSERT') THEN
    INSERT INTO audit_log (
      table_name, operation, record_id, new_data,
      changed_by, changed_ip, user_agent,
      legal_basis, compliance_framework
    ) VALUES (
      TG_TABLE_NAME, 'INSERT', NEW.id, to_jsonb(NEW),
      username, client_ip, user_agent,
      legal_basis, 'gdpr'
    );
    RETURN NEW;
    
  END IF;
  
  RETURN NULL;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Create triggers for security-critical tables
CREATE TRIGGER audit_domains
AFTER INSERT OR UPDATE OR DELETE ON domains
FOR EACH ROW EXECUTE FUNCTION create_audit_log();

CREATE TRIGGER audit_contacts
AFTER INSERT OR UPDATE OR DELETE ON contacts
FOR EACH ROW EXECUTE FUNCTION create_audit_log();

-- Index for compliance reporting
CREATE INDEX idx_audit_log_compliance ON audit_log(compliance_framework, changed_at);
CREATE INDEX idx_audit_log_legal_basis ON audit_log(legal_basis, table_name);
```

### 2. PII Access Monitoring
```sql
-- MySQL trigger for PII access monitoring
DELIMITER $$

CREATE TRIGGER monitor_pii_access
AFTER SELECT ON contacts
FOR EACH ROW
BEGIN
  DECLARE current_user VARCHAR(255);
  DECLARE client_ip VARCHAR(45);
  DECLARE access_purpose VARCHAR(50);
  
  -- Get session variables
  SET current_user = COALESCE(@app_current_user, USER());
  SET client_ip = COALESCE(@app_client_ip, SUBSTRING_INDEX(USER(), '@', -1));
  SET access_purpose = COALESCE(@app_access_purpose, 'unknown');
  
  -- Log PII access
  INSERT INTO pii_access_log (
    table_name, record_id, accessed_fields,
    user_name, client_ip, access_purpose,
    compliance_framework, legal_basis
  ) VALUES (
    'contacts', NEW.id, 
    -- Determine which PII fields were accessed
    JSON_ARRAY(
      IF(NEW.email IS NOT NULL, 'email', NULL),
      IF(NEW.phone IS NOT NULL, 'phone', NULL),
      IF(NEW.address IS NOT NULL, 'address', NULL)
    ),
    current_user, client_ip, access_purpose,
    'gdpr', 'legitimate-interest'
  );
  
EXCEPTION
  WHEN OTHERS THEN
    -- Never fail queries due to logging issues
    SIGNAL SQLSTATE '01000' SET MESSAGE_TEXT = 'PII access logging failed but query succeeded';
END$$

DELIMITER ;
```

### 3. Compliance Check Triggers
```sql
-- PostgreSQL trigger for GDPR compliance checks
CREATE OR REPLACE FUNCTION check_gdpr_compliance()
RETURNS TRIGGER AS $$
DECLARE
  data_category TEXT;
  retention_days INTEGER;
  legal_basis TEXT;
BEGIN
  -- Determine data category
  data_category := CASE
    WHEN NEW.contains_pii THEN 'personal'
    WHEN NEW.business_critical THEN 'business'
    ELSE 'public'
  END;
  
  -- Get retention policy
  SELECT policy.retention_days, policy.legal_basis
  INTO retention_days, legal_basis
  FROM data_retention_policies policy
  WHERE policy.data_category = data_category
  AND policy.compliance_framework = 'gdpr'
  LIMIT 1;
  
  IF NOT FOUND THEN
    RAISE EXCEPTION 'No GDPR retention policy found for category %', data_category;
  END IF;
  
  -- Set metadata
  NEW.retention_until := NOW() + INTERVAL '1 day' * retention_days;
  NEW.gdpr_legal_basis := legal_basis;
  NEW.gdpr_data_category := data_category;
  
  -- Check processing purpose
  IF NEW.processing_purpose IS NULL THEN
    RAISE EXCEPTION 'GDPR requires processing purpose for all personal data';
  END IF;
  
  -- Log compliance check
  INSERT INTO compliance_checks (
    table_name, record_id, 
    compliance_framework, data_category,
    retention_days, legal_basis,
    processing_purpose, check_time
  ) VALUES (
    TG_TABLE_NAME, NEW.id,
    'gdpr', data_category,
    retention_days, legal_basis,
    NEW.processing_purpose, NOW()
  );
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger
CREATE TRIGGER trigger_gdpr_compliance
BEFORE INSERT OR UPDATE ON domains
FOR EACH ROW EXECUTE FUNCTION check_gdpr_compliance();

-- Retention policies table
CREATE TABLE data_retention_policies (
  id SERIAL PRIMARY KEY,
  data_category VARCHAR(20) NOT NULL CHECK (data_category IN ('personal', 'business', 'public')),
  compliance_framework VARCHAR(20) NOT NULL CHECK (compliance_framework IN ('gdpr', 'ccpa', 'hipaa')),
  retention_days INTEGER NOT NULL,
  legal_basis VARCHAR(50) NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Sample policies
INSERT INTO data_retention_policies (data_category, compliance_framework, retention_days, legal_basis) VALUES
('personal', 'gdpr', 30, 'legitimate-interest'),
('business', 'gdpr', 2555, 'contract'),
('public', 'gdpr', 365, 'legitimate-interest');
```

---

## ‚ö° Performance Optimization

### 1. Conditional Trigger Execution
```sql
-- PostgreSQL conditional trigger
CREATE OR REPLACE FUNCTION conditional_domain_processing()
RETURNS TRIGGER AS $$
BEGIN
  -- Skip processing for internal system updates
  IF current_setting('app.system_update', true) = 'true' THEN
    RETURN NEW;
  END IF;
  
  -- Skip processing for low-priority updates
  IF NEW.update_priority = 'low' AND EXISTS (
    SELECT 1 FROM system_load WHERE load_level > 0.8
  ) THEN
    NEW.deferred_processing = true;
    RETURN NEW;
  END IF;
  
  -- Process high-priority updates immediately
  IF NEW.update_priority = 'high' THEN
    PERFORM process_high_priority_domain(NEW.id);
  END IF;
  
  -- Only process PII redaction for new records or PII changes
  IF TG_OP = 'INSERT' OR OLD.personal_data IS DISTINCT FROM NEW.personal_data THEN
    PERFORM redact_personal_data(NEW.id);
  END IF;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger with condition
CREATE TRIGGER trigger_conditional_processing
BEFORE INSERT OR UPDATE ON domains
FOR EACH ROW
WHEN (NEW.requires_processing = true)
EXECUTE FUNCTION conditional_domain_processing();
```

### 2. Batch Processing Optimization
```sql
-- PostgreSQL statement-level trigger for batch operations
CREATE OR REPLACE FUNCTION batch_domain_processing()
RETURNS TRIGGER AS $$
DECLARE
  domain_ids UUID[];
  batch_size INTEGER := 100;
  processed_count INTEGER := 0;
BEGIN
  -- Collect all domain IDs from the batch
  domain_ids := ARRAY(
    SELECT id FROM new_table
  );
  
  -- Process in batches to avoid long transactions
  WHILE processed_count < array_length(domain_ids, 1) LOOP
    PERFORM process_domain_batch(
      domain_ids[processed_count + 1:LEAST(processed_count + batch_size, array_length(domain_ids, 1))]
    );
    
    processed_count := processed_count + batch_size;
    COMMIT; -- Commit each batch
  END LOOP;
  
  RETURN NULL; -- Statement-level trigger doesn't return rows
END;
$$ LANGUAGE plpgsql;

-- Create statement-level trigger
CREATE TRIGGER trigger_batch_processing
AFTER INSERT OR UPDATE ON domains
REFERENCING NEW TABLE AS new_table
FOR EACH STATEMENT
EXECUTE FUNCTION batch_domain_processing();
```

### 3. Indexing for Trigger Performance
```sql
-- Critical indexes for trigger performance
CREATE INDEX CONCURRENTLY idx_domains_requires_processing ON domains(requires_processing) WHERE requires_processing = true;
CREATE INDEX CONCURRENTLY idx_domains_update_priority ON domains(update_priority, created_at);
CREATE INDEX CONCURRENTLY idx_domains_deferred_processing ON domains(deferred_processing) WHERE deferred_processing = true;
CREATE INDEX CONCURRENTLY idx_contacts_contains_pii ON contacts(contains_pii) WHERE contains_pii = true;
CREATE INDEX CONCURRENTLY idx_audit_log_timestamp ON audit_log(changed_at);
CREATE INDEX CONCURRENTLY idx_compliance_checks_data_category ON compliance_checks(data_category, compliance_framework);
CREATE INDEX CONCURRENTLY idx_pii_access_log_timestamp ON pii_access_log(accessed_at);

-- Partial indexes for GDPR compliance
CREATE INDEX CONCURRENTLY idx_domains_gdpr_personal ON domains(retention_until) 
WHERE gdpr_data_category = 'personal';

CREATE INDEX CONCURRENTLY idx_domains_business_critical ON domains(created_at) 
WHERE business_critical = true;

-- BRIN indexes for time-series data
CREATE INDEX CONCURRENTLY idx_audit_log_brin ON audit_log USING BRIN (changed_at) WITH (pages_per_range = 32);
CREATE INDEX CONCURRENTLY idx_compliance_checks_brin ON compliance_checks USING BRIN (check_time) WITH (pages_per_range = 32);
```

---

## üöÄ Advanced Patterns

### 1. Event-Driven Architecture with Triggers
```sql
-- PostgreSQL trigger for event production
CREATE OR REPLACE FUNCTION produce_domain_events()
RETURNS TRIGGER AS $$
DECLARE
  event_type TEXT;
  event_data JSONB;
BEGIN
  -- Determine event type
  IF TG_OP = 'INSERT' THEN
    event_type := 'domain.created';
  ELSIF TG_OP = 'UPDATE' THEN
    IF OLD.expiration_date IS DISTINCT FROM NEW.expiration_date THEN
      event_type := 'domain.expiration_updated';
    ELSIF OLD.status IS DISTINCT FROM NEW.status THEN
      event_type := 'domain.status_updated';
    ELSE
      event_type := 'domain.updated';
    END IF;
  ELSIF TG_OP = 'DELETE' THEN
    event_type := 'domain.deleted';
  END IF;
  
  -- Build event data
  event_data := jsonb_build_object(
    'domain_id', NEW.id,
    'domain_name', NEW.domain_name,
    'operation', TG_OP,
    'timestamp', NOW(),
    'user_agent', current_setting('app.user_agent', true),
    'source_ip', current_setting('app.client_ip', true),
    'previous_state', CASE WHEN TG_OP IN ('UPDATE', 'DELETE') THEN to_jsonb(OLD) ELSE NULL END,
    'current_state', CASE WHEN TG_OP IN ('INSERT', 'UPDATE') THEN to_jsonb(NEW) ELSE NULL END
  );
  
  -- Insert into events table for consumption
  INSERT INTO domain_events (event_type, event_data, processed)
  VALUES (event_type, event_data, false);
  
  -- Also send to notification system for critical events
  IF event_type IN ('domain.expiration_updated', 'domain.status_updated') AND 
     current_setting('app.notifications_enabled', true) = 'true' THEN
    
    PERFORM pg_notify('domain_notifications', jsonb_build_object(
      'event_type', event_type,
      'domain_name', NEW.domain_name,
      'priority', CASE WHEN NEW.business_critical THEN 'high' ELSE 'medium' END
    )::text);
  END IF;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger
CREATE TRIGGER trigger_domain_events
AFTER INSERT OR UPDATE OR DELETE ON domains
FOR EACH ROW EXECUTE FUNCTION produce_domain_events();

-- Events table
CREATE TABLE domain_events (
  id BIGSERIAL PRIMARY KEY,
  event_type TEXT NOT NULL,
  event_data JSONB NOT NULL,
  processed BOOLEAN DEFAULT false,
  processed_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index for event consumers
CREATE INDEX idx_domain_events_unprocessed ON domain_events(processed) WHERE processed = false;
CREATE INDEX idx_domain_events_type ON domain_events(event_type);
```

### 2. Cross-Table Data Synchronization
```sql
-- PostgreSQL trigger for cross-table synchronization
CREATE OR REPLACE FUNCTION sync_domain_contacts()
RETURNS TRIGGER AS $$
BEGIN
  -- Handle contact updates
  IF TG_OP = 'UPDATE' THEN
    -- Update references in domain_contacts
    UPDATE domain_contacts dc
    SET contact_data = NEW.contact_data,
        updated_at = NOW()
    WHERE dc.contact_id = NEW.id
    AND dc.contact_data IS DISTINCT FROM NEW.contact_data;
    
    -- If primary contact changes, update domain metadata
    IF NEW.is_primary = true AND OLD.is_primary = false THEN
      UPDATE domains d
      SET primary_contact_id = NEW.id,
          updated_at = NOW()
      WHERE d.id IN (
        SELECT domain_id FROM domain_contacts WHERE contact_id = NEW.id
      );
    END IF;
  END IF;
  
  -- Handle contact deletion
  IF TG_OP = 'DELETE' THEN
    -- Reassign domains to backup contacts
    UPDATE domains d
    SET primary_contact_id = (
      SELECT contact_id 
      FROM domain_contacts 
      WHERE domain_id = d.id 
      AND contact_id != OLD.id
      ORDER BY created_at DESC
      LIMIT 1
    ),
    updated_at = NOW()
    WHERE d.primary_contact_id = OLD.id;
    
    -- Delete orphaned domain_contacts
    DELETE FROM domain_contacts
    WHERE contact_id = OLD.id;
  END IF;
  
  RETURN NULL; -- For statement-level trigger
END;
$$ LANGUAGE plpgsql;

-- Create statement-level trigger
CREATE TRIGGER trigger_sync_domain_contacts
AFTER UPDATE OR DELETE ON contacts
REFERENCING NEW TABLE AS new_table OLD TABLE AS old_table
FOR EACH STATEMENT
EXECUTE FUNCTION sync_domain_contacts();
```

### 3. Temporal Data Versioning
```sql
-- PostgreSQL history table for temporal queries
CREATE TABLE domains_history (
  id UUID NOT NULL,
  domain_name TEXT NOT NULL,
  registrar_id INTEGER,
  nameservers JSONB,
  status VARCHAR(50)[],
  expiration_date TIMESTAMPTZ,
  valid_from TIMESTAMPTZ NOT NULL,
  valid_to TIMESTAMPTZ,
  deleted BOOLEAN DEFAULT false,
  version INTEGER NOT NULL,
  changed_by TEXT,
  changed_at TIMESTAMPTZ DEFAULT NOW()
);

-- Trigger for temporal versioning
CREATE OR REPLACE FUNCTION version_domain_history()
RETURNS TRIGGER AS $$
BEGIN
  -- End previous version
  UPDATE domains_history
  SET valid_to = NOW()
  WHERE id = NEW.id AND valid_to IS NULL;
  
  -- Insert new version
  INSERT INTO domains_history (
    id, domain_name, registrar_id, nameservers, status, expiration_date,
    valid_from, version, changed_by
  ) VALUES (
    NEW.id, NEW.domain_name, NEW.registrar_id, NEW.nameservers, NEW.status, NEW.expiration_date,
    NOW(), COALESCE((SELECT MAX(version) FROM domains_history WHERE id = NEW.id), 0) + 1,
    current_setting('app.current_user', true)
  );
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Create trigger
CREATE TRIGGER trigger_domain_versioning
AFTER INSERT OR UPDATE ON domains
FOR EACH ROW EXECUTE FUNCTION version_domain_history();

-- Soft delete trigger
CREATE OR REPLACE FUNCTION soft_delete_domains()
RETURNS TRIGGER AS $$
BEGIN
  -- Don't actually delete, mark as deleted
  UPDATE domains_history
  SET valid_to = NOW(), deleted = true
  WHERE id = OLD.id AND valid_to IS NULL;
  
  RETURN OLD;
END;
$$ LANGUAGE plpgsql;

-- Create delete trigger
CREATE TRIGGER trigger_soft_delete_domains
INSTEAD OF DELETE ON domains
FOR EACH ROW EXECUTE FUNCTION soft_delete_domains();

-- Indexes for temporal queries
CREATE INDEX idx_domains_history_id_valid_to ON domains_history(id, valid_to);
CREATE INDEX idx_domains_history_valid_period ON domains_history USING gist (tsrange(valid_from, valid_to));
CREATE INDEX idx_domains_history_deleted ON domains_history(deleted) WHERE deleted = true;
```

---

## üß™ Testing & Validation

### 1. Trigger Testing Framework
```javascript
// trigger-test-framework.js
const { Pool } = require('pg');
const { expect } = require('chai');

class TriggerTestFramework {
  constructor() {
    this.pool = new Pool({
      connectionString: process.env.TEST_DATABASE_URL
    });
    this.testData = new Map();
  }
  
  async setup() {
    // Create test schema
    await this.pool.query(`
      CREATE SCHEMA IF NOT EXISTS trigger_tests;
      SET search_path TO trigger_tests, public;
    `);
    
    // Create test tables
    await this.pool.query(`
      CREATE TABLE domains (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        domain_name TEXT NOT NULL,
        registrar_id INTEGER,
        nameservers JSONB DEFAULT '[]'::jsonb,
        status VARCHAR(50)[],
        expiration_date TIMESTAMPTZ,
        created_at TIMESTAMPTZ DEFAULT NOW(),
        updated_at TIMESTAMPTZ DEFAULT NOW()
      );
      
      CREATE TABLE contacts (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        email TEXT,
        phone TEXT,
        name TEXT,
        created_at TIMESTAMPTZ DEFAULT NOW()
      );
      
      CREATE TABLE audit_log (
        id BIGSERIAL PRIMARY KEY,
        table_name TEXT NOT NULL,
        operation TEXT NOT NULL,
        record_id UUID,
        old_data JSONB,
        new_data JSONB,
        changed_at TIMESTAMPTZ DEFAULT NOW()
      );
    `);
    
    // Install triggers to test
    await this.installTestTriggers();
  }
  
  async installTestTriggers() {
    // Install test versions of triggers
    await this.pool.query(`
      CREATE OR REPLACE FUNCTION test_audit_trigger()
      RETURNS TRIGGER AS $$
      BEGIN
        INSERT INTO audit_log (
          table_name, operation, record_id, old_data, new_data
        ) VALUES (
          TG_TABLE_NAME, TG_OP,
          COALESCE(NEW.id, OLD.id),
          CASE WHEN TG_OP IN ('UPDATE', 'DELETE') THEN to_jsonb(OLD) ELSE NULL END,
          CASE WHEN TG_OP IN ('INSERT', 'UPDATE') THEN to_jsonb(NEW) ELSE NULL END
        );
        RETURN COALESCE(NEW, OLD);
      END;
      $$ LANGUAGE plpgsql;
      
      CREATE TRIGGER trigger_domains_audit
      AFTER INSERT OR UPDATE OR DELETE ON domains
      FOR EACH ROW EXECUTE FUNCTION test_audit_trigger();
      
      CREATE TRIGGER trigger_contacts_audit
      AFTER INSERT OR UPDATE OR DELETE ON contacts
      FOR EACH ROW EXECUTE FUNCTION test_audit_trigger();
    `);
  }
  
  async testTriggerScenarios() {
    console.log('üß™ Testing trigger scenarios...');
    
    // Test 1: Basic insert audit
    const testDomain = await this.testBasicInsert();
    
    // Test 2: Update with old data capture
    await this.testUpdateAudit(testDomain.id);
    
    // Test 3: Delete with full history
    await this.testDeleteAudit(testDomain.id);
    
    // Test 4: Bulk insert performance
    await this.testBulkInsertPerformance(1000);
    
    // Test 5: Error handling
    await this.testErrorHandling();
    
    console.log('‚úÖ All trigger tests passed');
  }
  
  async testBasicInsert() {
    const { rows } = await this.pool.query(`
      INSERT INTO domains (domain_name, registrar_id)
      VALUES ('example.com', 123)
      RETURNING id, domain_name;
    `);
    
    const domain = rows[0];
    
    // Verify audit log
    const { rows: auditRows } = await this.pool.query(`
      SELECT * FROM audit_log 
      WHERE table_name = 'domains' AND operation = 'INSERT'
      AND record_id = $1;
    `, [domain.id]);
    
    expect(auditRows).to.have.length(1);
    expect(auditRows[0].new_data->>'domain_name').to.equal('example.com');
    expect(auditRows[0].operation).to.equal('INSERT');
    
    return domain;
  }
  
  async testUpdateAudit(domainId) {
    await this.pool.query(`
      UPDATE domains 
      SET status = ARRAY['active', 'locked'], expiration_date = NOW() + INTERVAL '1 year'
      WHERE id = $1;
    `, [domainId]);
    
    const { rows } = await this.pool.query(`
      SELECT * FROM audit_log 
      WHERE table_name = 'domains' AND operation = 'UPDATE'
      AND record_id = $1
      ORDER BY changed_at DESC
      LIMIT 1;
    `, [domainId]);
    
    expect(rows).to.have.length(1);
    expect(rows[0].old_data->'status').to.be.null;
    expect(rows[0].new_data->'status').to.not.be.null;
    expect(rows[0].new_data->>'status').to.include('active');
  }
  
  async testDeleteAudit(domainId) {
    await this.pool.query('DELETE FROM domains WHERE id = $1;', [domainId]);
    
    const { rows } = await this.pool.query(`
      SELECT * FROM audit_log 
      WHERE table_name = 'domains' AND operation = 'DELETE'
      AND record_id = $1;
    `, [domainId]);
    
    expect(rows).to.have.length(1);
    expect(rows[0].old_data->>'domain_name').to.equal('example.com');
    expect(rows[0].new_data).to.be.null;
  }
  
  async testBulkInsertPerformance(count) {
    console.time(`Bulk insert ${count} domains`);
    
    const values = Array(count).fill(0).map((_, i) => 
      `('domain${i}.com', ${i % 10 + 1})`
    ).join(', ');
    
    await this.pool.query(`
      INSERT INTO domains (domain_name, registrar_id)
      VALUES ${values};
    `);
    
    console.timeEnd(`Bulk insert ${count} domains`);
    
    // Verify audit log count
    const { rows } = await this.pool.query(`
      SELECT COUNT(*) as count FROM audit_log 
      WHERE table_name = 'domains' AND operation = 'INSERT'
      AND changed_at > NOW() - INTERVAL '5 minutes';
    `);
    
    expect(parseInt(rows[0].count)).to.equal(count);
  }
  
  async testErrorHandling() {
    try {
      // Force an error in the trigger
      await this.pool.query(`
        INSERT INTO domains (domain_name, registrar_id)
        VALUES (NULL, 123);
      `);
      throw new Error('Expected error not thrown');
    } catch (error) {
      expect(error.message).to.include('null value in column "domain_name"');
      
      // Verify no partial audit log was created
      const { rows } = await this.pool.query(`
        SELECT COUNT(*) as count FROM audit_log 
        WHERE changed_at > NOW() - INTERVAL '1 minute';
      `);
      
      expect(parseInt(rows[0].count)).to.equal(0);
    }
  }
  
  async teardown() {
    await this.pool.query('DROP SCHEMA IF EXISTS trigger_tests CASCADE;');
    await this.pool.end();
  }
}

// Run tests
async function runTriggerTests() {
  const framework = new TriggerTestFramework();
  
  try {
    await framework.setup();
    await framework.testTriggerScenarios();
  } finally {
    await framework.teardown();
  }
}

module.exports = { runTriggerTests };
```

### 2. Performance Benchmarking
```sql
-- PostgreSQL performance benchmark for triggers
CREATE TABLE trigger_performance (
  id BIGSERIAL PRIMARY KEY,
  test_name TEXT NOT NULL,
  operation TEXT NOT NULL,
  row_count INTEGER NOT NULL,
  duration_ms NUMERIC NOT NULL,
  avg_duration_ms NUMERIC NOT NULL,
  trigger_overhead_percent NUMERIC NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Benchmark function
CREATE OR REPLACE FUNCTION benchmark_triggers(test_rows INTEGER)
RETURNS TABLE (
  test_name TEXT,
  operation TEXT,
  row_count INTEGER,
  duration_ms NUMERIC,
  avg_duration_ms NUMERIC,
  trigger_overhead_percent NUMERIC
) AS $$
DECLARE
  start_time TIMESTAMPTZ;
  end_time TIMESTAMPTZ;
  duration_ms NUMERIC;
  baseline_duration_ms NUMERIC;
BEGIN
  -- Create test data
  CREATE TEMP TABLE test_domains (LIKE domains INCLUDING DEFAULTS);
  
  -- Insert baseline data without triggers
  EXECUTE format('
    INSERT INTO test_domains (domain_name, registrar_id)
    SELECT 
      ''domain'' || i || ''.com'',
      (i % 10) + 1
    FROM generate_series(1, %s) i
  ', test_rows);
  
  -- Baseline measurement (no triggers)
  start_time := clock_timestamp();
  
  EXECUTE format('
    UPDATE test_domains 
    SET status = ARRAY[''active''], expiration_date = NOW() + INTERVAL ''1 year''
  ');
  
  end_time := clock_timestamp();
  baseline_duration_ms := EXTRACT(EPOCH FROM (end_time - start_time)) * 1000;
  
  -- Measurement with triggers
  CREATE TEMP TABLE test_domains_with_triggers (LIKE domains INCLUDING DEFAULTS);
  CREATE TRIGGER trigger_test_audit
  AFTER INSERT OR UPDATE ON test_domains_with_triggers
  FOR EACH ROW EXECUTE FUNCTION test_audit_trigger();
  
  EXECUTE format('
    INSERT INTO test_domains_with_triggers (domain_name, registrar_id)
    SELECT 
      ''domain'' || i || ''.com'',
      (i % 10) + 1
    FROM generate_series(1, %s) i
  ', test_rows);
  
  start_time := clock_timestamp();
  
  EXECUTE format('
    UPDATE test_domains_with_triggers 
    SET status = ARRAY[''active''], expiration_date = NOW() + INTERVAL ''1 year''
  ');
  
  end_time := clock_timestamp();
  duration_ms := EXTRACT(EPOCH FROM (end_time - start_time)) * 1000;
  
  -- Calculate overhead
  trigger_overhead_percent := (duration_ms - baseline_duration_ms) / baseline_duration_ms * 100;
  
  -- Return results
  RETURN QUERY SELECT
    'audit_trigger'::TEXT,
    'update'::TEXT,
    test_rows,
    duration_ms,
    duration_ms / test_rows,
    trigger_overhead_percent;
  
  -- Cleanup
  DROP TABLE test_domains;
  DROP TABLE test_domains_with_triggers;
END;
$$ LANGUAGE plpgsql;

-- Run benchmarks
INSERT INTO trigger_performance 
SELECT * FROM benchmark_triggers(1000);

INSERT INTO trigger_performance 
SELECT * FROM benchmark_triggers(10000);

INSERT INTO trigger_performance 
SELECT * FROM benchmark_triggers(100000);
```

### 3. Chaos Engineering for Triggers
```yaml
# chaos-experiment-triggers.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
meta
  name: database-high-load
  namespace: rdap-production
spec:
  action: pod-failure
  mode: one
  value: ""
  selector:
    labelSelectors:
      app: postgresql-primary
  duration: "30s"
---
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
meta
  name: trigger-latency-spike
  namespace: rdap-production
spec:
  action: delay
  mode: all
  selector:
    labelSelectors:
      app: postgresql-primary
  delay:
    latency: "100ms"
    correlation: "25"
    jitter: "50ms"
  direction: to
  target:
    selector:
      labelSelectors:
        app: audit-processor
    mode: all
---
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
meta
  name: cpu-stress-test
  namespace: rdap-production
spec:
  mode: one
  selector:
    labelSelectors:
      app: postgresql-primary
  stressors:
    cpu:
      workers: 4
      load: 80
  duration: "2m"
```

```bash
# Run chaos experiments
kubectl apply -f chaos-experiment-triggers.yaml

# Monitor trigger behavior during chaos
watch -n 1 "psql -c \"SELECT COUNT(*) FROM audit_log WHERE changed_at > NOW() - INTERVAL '1 minute';\""

# Monitor trigger queue backlog
watch -n 1 "psql -c \"SELECT COUNT(*) FROM domain_events WHERE processed = false;\""

# After experiment, analyze data loss
psql -c "
  SELECT 
    COUNT(*) as total_events,
    COUNT(*) FILTER (WHERE processed = true) as processed_events,
    COUNT(*) FILTER (WHERE processed = false AND created_at < NOW() - INTERVAL '5 minutes') as stale_events
  FROM domain_events
  WHERE created_at > NOW() - INTERVAL '1 hour';
"
```

---

## üîç Monitoring & Observability

### 1. Prometheus Metrics
```sql
-- PostgreSQL metrics for Prometheus
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Function to expose trigger metrics
CREATE OR REPLACE FUNCTION get_trigger_metrics()
RETURNS TABLE (
  trigger_name TEXT,
  calls BIGINT,
  total_time NUMERIC,
  avg_time NUMERIC,
  rows_processed BIGINT,
  error_count BIGINT
) AS $$
BEGIN
  RETURN QUERY
  SELECT 
    proname as trigger_name,
    calls,
    total_time,
    total_time / NULLIF(calls, 0) as avg_time,
    rows as rows_processed,
    0 as error_count  -- Error tracking requires additional logging
  FROM pg_stat_user_functions
  WHERE proname LIKE 'trigger_%'
  ORDER BY total_time DESC;
END;
$$ LANGUAGE plpgsql;

-- Create metrics view
CREATE MATERIALIZED VIEW trigger_metrics AS
SELECT * FROM get_trigger_metrics();

-- Refresh every minute
SELECT cron.schedule('refresh_trigger_metrics', '*/1 * * * *', $$REFRESH MATERIALIZED VIEW trigger_metrics$$);
```

### 2. Grafana Dashboard Configuration
```json
{
  "title": "Database Trigger Performance",
  "panels": [
    {
      "title": "Trigger Execution Count",
      "type": "graph",
      "targets": [
        {
          "expr": "sum by (trigger_name) (pg_stat_user_functions{datname=\"rdap_production\"})",
          "legendFormat": "{{trigger_name}}"
        }
      ],
      "yAxis": {
        "label": "Executions per second"
      }
    },
    {
      "title": "Average Trigger Duration",
      "type": "graph",
      "targets": [
        {
          "expr": "avg by (trigger_name) (pg_stat_user_functions_total_time{datname=\"rdap_production\"} / pg_stat_user_functions_calls{datname=\"rdap_production\"})",
          "legendFormat": "{{trigger_name}}"
        }
      ],
      "yAxis": {
        "label": "Milliseconds",
        "min": 0
      }
    },
    {
      "title": "Queue Backlog",
      "type": "stat",
      "targets": [
        {
          "expr": "sum(pg_stat_user_tables_n_tup_ins{table=\"domain_events\", datname=\"rdap_production\"}) - sum(pg_stat_user_tables_n_tup_del{table=\"domain_events\", datname=\"rdap_production\"})",
          "legendFormat": "Unprocessed Events"
        }
      ],
      "options": {
        "colorMode": "value",
        "thresholds": [
          {"value": 1000, "color": "green"},
          {"value": 5000, "color": "yellow"},
          {"value": 10000, "color": "red"}
        ]
      }
    },
    {
      "title": "Error Rate",
      "type": "graph",
      "targets": [
        {
          "expr": "sum by (trigger_name) (rate(trigger_errors_total{datname=\"rdap_production\"}[5m]))",
          "legendFormat": "{{trigger_name}}"
        }
      ],
      "yAxis": {
        "label": "Errors per second"
      }
    }
  ],
  "templating": {
    "list": [
      {
        "name": "database",
        "query": "label_values(pg_stat_database_datname)",
        "current": "rdap_production"
      }
    ]
  }
}
```

### 3. OpenTelemetry Integration
```javascript
// otel-trigger-instrumentation.js
const opentelemetry = require('@opentelemetry/api');
const { trace, context } = opentelemetry;
const { SpanKind } = require('@opentelemetry/api');

class TriggerInstrumentation {
  constructor(pool) {
    this.pool = pool;
    this.tracer = opentelemetry.trace.getTracer('rdap-triggers');
  }
  
  async monitorTriggers() {
    // Create span for trigger monitoring
    const span = this.tracer.startSpan('trigger.monitoring', {
      kind: SpanKind.SERVER,
      attributes: {
        'database.system': 'postgresql',
        'service.name': 'rdap-trigger-monitor'
      }
    });
    
    const ctx = trace.setSpan(context.active(), span);
    
    try {
      // Monitor long-running triggers
      const { rows: longTriggers } = await context.with(ctx, async () => {
        return this.pool.query(`
          SELECT 
            pid,
            now() - query_start as duration,
            query
          FROM pg_stat_activity
          WHERE query LIKE 'trigger_%'
          AND now() - query_start > INTERVAL '5 seconds'
          ORDER BY duration DESC;
        `);
      });
      
      // Create spans for each long-running trigger
      longTriggers.forEach(trigger => {
        const triggerSpan = this.tracer.startSpan('trigger.long_running', {
          kind: SpanKind.INTERNAL,
          parent: span,
          attributes: {
            'trigger.pid': trigger.pid,
            'trigger.duration': trigger.duration.toString(),
            'trigger.query': trigger.query.substring(0, 100)
          }
        });
        
        triggerSpan.setAttribute('trigger.slow', true);
        triggerSpan.end();
      });
      
      // Monitor trigger queue backlog
      const { rows: queueStats } = await context.with(ctx, async () => {
        return this.pool.query(`
          SELECT 
            COUNT(*) FILTER (WHERE processed = false) as unprocessed,
            COUNT(*) FILTER (WHERE processed = false AND created_at < NOW() - INTERVAL '5 minutes') as stale
          FROM domain_events;
        `);
      });
      
      span.setAttribute('trigger.unprocessed_count', parseInt(queueStats[0].unprocessed));
      span.setAttribute('trigger.stale_count', parseInt(queueStats[0].stale));
      
      // Alert if backlog is growing
      if (parseInt(queueStats[0].unprocessed) > 10000) {
        span.setAttribute('trigger.backlog_alert', true);
        this.sendAlert('Trigger backlog exceeding threshold');
      }
      
    } catch (error) {
      span.recordException(error);
      throw error;
    } finally {
      span.end();
    }
  }
  
  sendAlert(message) {
    // Send alert to monitoring system
    console.error(`[TRIGGER ALERT] ${message}`);
    
    // In production, this would integrate with PagerDuty, Slack, etc.
    this.tracer.getSpan(context.active()).setAttribute('trigger.alert_sent', true);
  }
  
  setupContinuousMonitoring(interval = 60000) {
    setInterval(() => this.monitorTriggers(), interval);
    console.log('‚úÖ Trigger monitoring started');
  }
}

// Usage
const { Pool } = require('pg');
const pool = new Pool({ connectionString: process.env.DATABASE_URL });

const instrumentation = new TriggerInstrumentation(pool);
instrumentation.setupContinuousMonitoring();
```

---

## üõ°Ô∏è Production Deployment Template

### 1. AWS RDS with Trigger Optimization
```yaml
# rds-triggers-deployment.yaml
AWSTemplateFormatVersion: '2010-09-09'
Resources:
  RDAPDatabase:
    Type: AWS::RDS::DBInstance
    Properties:
      DBInstanceIdentifier: rdap-production
      Engine: postgres
      EngineVersion: "15.4"
      DBInstanceClass: db.r6g.2xlarge
      AllocatedStorage: 500
      Iops: 3000
      StorageType: io1
      StorageEncrypted: true
      KmsKeyId: !Ref EncryptionKey
      BackupRetentionPeriod: 35
      MultiAZ: true
      PubliclyAccessible: false
      VPCSecurityGroups:
        - !Ref DatabaseSecurityGroup
      DBSubnetGroupName: !Ref DBSubnetGroup
      EnablePerformanceInsights: true
      PerformanceInsightsRetentionPeriod: 7
      PerformanceInsightsKMSKeyId: !Ref EncryptionKey
      Parameters:
        - ParameterName: max_connections
          ParameterValue: "1000"
        - ParameterName: shared_preload_libraries
          ParameterValue: "pg_stat_statements,pg_cron,pg_partman_bgw"
        - ParameterName: pg_stat_statements.track
          ParameterValue: "all"
        - ParameterName: cron.database_name
          ParameterValue: "rdap_production"
        - ParameterName: work_mem
          ParameterValue: "64MB"
        - ParameterName: maintenance_work_mem
          ParameterValue: "256MB"
        - ParameterName: effective_cache_size
          ParameterValue: "6GB"
        - ParameterName: checkpoint_completion_target
          ParameterValue: "0.9"
        - ParameterName: wal_buffers
          ParameterValue: "16MB"
        - ParameterName: default_statistics_target
          ParameterValue: "100"
        
      DBClusterParameterGroupName: !Ref DBParameterGroup
  
  DBParameterGroup:
    Type: AWS::RDS::DBParameterGroup
    Properties:
      Family: postgres15
      Description: RDAP production parameter group
      Parameters:
        - ParameterName: log_statement
          ParameterValue: "ddl"
        - ParameterName: log_min_duration_statement
          ParameterValue: "1000"
        - ParameterName: track_functions
          ParameterValue: "pl"
        - ParameterName: autovacuum_vacuum_scale_factor
          ParameterValue: "0.01"
        - ParameterName: autovacuum_analyze_scale_factor
          ParameterValue: "0.005"
        - ParameterName: autovacuum_vacuum_cost_limit
          ParameterValue: "2000"
  
  DatabaseSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: RDAP database security group
      VpcId: !Ref VPC
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 5432
          ToPort: 5432
          SourceSecurityGroupId: !Ref ApplicationSecurityGroup
        - IpProtocol: tcp
          FromPort: 5432
          ToPort: 5432
          SourceSecurityGroupId: !Ref MonitoringSecurityGroup
  
  # Monitoring resources for trigger performance
  TriggerMonitoringDashboard:
    Type: AWS::CloudWatch::Dashboard
    Properties:
      DashboardName: RDAP-Trigger-Performance
      DashboardBody: !Sub |
        {
          "widgets": [
            {
              "type": "metric",
              "x": 0,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "RDSOSMetrics", "cpuUtilization", "DBInstanceIdentifier", "${RDAPDatabase}" ]
                ],
                "period": 300,
                "stat": "Average",
                "region": "${AWS::Region}",
                "title": "CPU Utilization"
              }
            },
            {
              "type": "metric",
              "x": 12,
              "y": 0,
              "width": 12,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "RDSOSMetrics", "loadAverageMinute", "DBInstanceIdentifier", "${RDAPDatabase}", { "stat": "Average" } ]
                ],
                "period": 300,
                "region": "${AWS::Region}",
                "title": "Load Average"
              }
            },
            {
              "type": "metric",
              "x": 0,
              "y": 6,
              "width": 24,
              "height": 6,
              "properties": {
                "metrics": [
                  [ "AWS/RDS", "DatabaseConnections", "DBInstanceIdentifier", "${RDAPDatabase}", { "stat": "Maximum" } ],
                  [".", "Transactions", ".", "${RDAPDatabase}", { "stat": "Sum" } ]
                ],
                "period": 300,
                "region": "${AWS::Region}",
                "title": "Database Activity"
              }
            }
          ]
        }
```

### 2. Trigger Security Hardening
```sql
-- PostgreSQL trigger security hardening
-- 1. Restrict trigger function permissions
REVOKE EXECUTE ON FUNCTION create_audit_log() FROM PUBLIC;
GRANT EXECUTE ON FUNCTION create_audit_log() TO rdap_app_user;

-- 2. Row-level security on audit tables
ALTER TABLE audit_log ENABLE ROW LEVEL SECURITY;

CREATE POLICY audit_log_access_policy ON audit_log
USING (
  current_user = 'data_protection_officer' OR
  current_user = 'compliance_officer' OR
  (current_user = 'rdap_app_user' AND changed_at > NOW() - INTERVAL '24 hours')
);

-- 3. Mask sensitive fields in audit logs
CREATE OR REPLACE FUNCTION mask_audit_log_data()
RETURNS TRIGGER AS $$
BEGIN
  -- Mask PII fields in audit logs
  IF NEW.old_data IS NOT NULL THEN
    NEW.old_data := NEW.old_data || jsonb_build_object(
      'email', CASE WHEN NEW.old_data ? 'email' THEN 'REDACTED@audit.invalid' ELSE NULL END,
      'phone', CASE WHEN NEW.old_data ? 'phone' THEN 'REDACTED' ELSE NULL END,
      'address', CASE WHEN NEW.old_data ? 'address' THEN 'REDACTED' ELSE NULL END
    );
  END IF;
  
  IF NEW.new_data IS NOT NULL THEN
    NEW.new_data := NEW.new_data || jsonb_build_object(
      'email', CASE WHEN NEW.new_data ? 'email' THEN 'REDACTED@audit.invalid' ELSE NULL END,
      'phone', CASE WHEN NEW.new_data ? 'phone' THEN 'REDACTED' ELSE NULL END,
      'address', CASE WHEN NEW.new_data ? 'address' THEN 'REDACTED' ELSE NULL END
    );
  END IF;
  
  RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

CREATE TRIGGER trigger_mask_audit_log
BEFORE INSERT ON audit_log
FOR EACH ROW EXECUTE FUNCTION mask_audit_log_data();

-- 4. Immutable audit logs
CREATE OR REPLACE FUNCTION prevent_audit_log_modification()
RETURNS TRIGGER AS $$
BEGIN
  IF TG_OP = 'UPDATE' THEN
    RAISE EXCEPTION 'Audit logs are immutable and cannot be modified';
  END IF;
  
  IF TG_OP = 'DELETE' THEN
    RAISE EXCEPTION 'Audit logs cannot be deleted';
  END IF;
  
  RETURN NULL;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_prevent_audit_modification
BEFORE UPDATE OR DELETE ON audit_log
FOR EACH ROW EXECUTE FUNCTION prevent_audit_log_modification();

-- 5. Audit log integrity verification
CREATE TABLE audit_log_checksums (
  calculation_time TIMESTAMPTZ PRIMARY KEY DEFAULT NOW(),
  row_count BIGINT NOT NULL,
  data_hash TEXT NOT NULL
);

CREATE OR REPLACE FUNCTION calculate_audit_log_checksum()
RETURNS VOID AS $$
BEGIN
  INSERT INTO audit_log_checksums (row_count, data_hash)
  SELECT 
    COUNT(*),
    encode(digest(string_agg(id::text || changed_at::text, ''), 'sha256'), 'hex')
  FROM audit_log
  WHERE changed_at > NOW() - INTERVAL '1 day';
END;
$$ LANGUAGE plpgsql;

-- Run hourly
SELECT cron.schedule('audit_log_checksum', '0 * * * *', $$SELECT calculate_audit_log_checksum()$$);
```

---

## üìö Related Documentation

| Document | Description | Path |
|----------|-------------|------|
| [Database Schemas](schemas.md) | RDAP database schema design | schemas.md |
| [Sync Tools](sync_tools.md) | Data synchronization patterns | sync_tools.md |
| [Security Privacy](../../guides/security_privacy.md) | Privacy-preserving trigger design | ../../guides/security_privacy.md |
| [Trigger Validator](./trigger-validator.md) | Trigger configuration validation tool | trigger-validator.md |
| [Performance Benchmarks](../../../benchmarks/results/trigger-performance.md) | Trigger performance benchmarks | ../../../benchmarks/results/trigger-performance.md |

---

## üè∑Ô∏è Trigger Specifications

| Property | Value |
|----------|-------|
| **Trigger Types Supported** | BEFORE/AFTER INSERT/UPDATE/DELETE, INSTEAD OF |
| **Database Compatibility** | PostgreSQL 12+, MySQL 8.0+, SQL Server 2019+ |
| **Performance Impact** | < 5ms per operation (optimized triggers) |
| **Maximum Throughput** | 10,000 operations/second (high-end hardware) |
| **GDPR Compliance** | ‚úÖ Article 6 (Lawful Basis), Article 17 (Erasure) |
| **CCPA Compliance** | ‚úÖ Section 1798.100 (Consumer Rights) |
| **Audit Trail** | 100% immutable record of data modifications |
| **Test Coverage** | 98% unit tests, 90% integration tests |
| **Last Updated** | December 5, 2025 |

> **üîê Critical Security Reminder:** Always run triggers with minimal privileges. Never execute dynamic SQL in triggers without rigorous input validation. Implement row-level security on audit tables and regularly verify trigger integrity through checksums. All PII in audit logs must be automatically redacted, and access to audit data should be strictly limited to authorized personnel.

[‚Üê Back to Database Integrations](../README.md) | [Next: Analytics Dashboard ‚Üí](../../../analytics/dashboard_components.md)

*Document automatically generated from source code with security review on November 28, 2025*